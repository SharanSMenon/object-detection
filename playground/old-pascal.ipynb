{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "import xml.etree.ElementTree as ET\n",
    "from PIL import Image\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOC_CLASSES = (  # always index 0\n",
    "    'aeroplane', 'bicycle', 'bird', 'boat',\n",
    "    'bottle', 'bus', 'car', 'cat', 'chair',\n",
    "    'cow', 'diningtable', 'dog', 'horse',\n",
    "    'motorbike', 'person', 'pottedplant',\n",
    "    'sheep', 'sofa', 'train', 'tvmonitor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VOCDetection(datasets.VisionDataset):\n",
    "    \"\"\"`Pascal VOC <http://host.robots.ox.ac.uk/pascal/VOC/>`_ Detection Dataset.\n",
    "    Args:\n",
    "        root (string): Root directory of the VOC Dataset.\n",
    "        year (string, optional): The dataset year, supports years 2007 to 2012.\n",
    "        image_set (string, optional): Select the image_set to use, ``train``, ``trainval`` or ``val``\n",
    "        download (bool, optional): If true, downloads the dataset from the internet and\n",
    "            puts it in root directory. If dataset is already downloaded, it is not\n",
    "            downloaded again.\n",
    "            (default: alphabetic indexing of VOC's 20 classes).\n",
    "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
    "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
    "        target_transform (callable, required): A function/transform that takes in the\n",
    "            target and transforms it.\n",
    "        transforms (callable, optional): A function/transform that takes input sample and its target as entry\n",
    "            and returns a transformed version.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            root: str,\n",
    "            year: str = \"2012\",\n",
    "            image_set: str = \"train\",\n",
    "            transform= None,\n",
    "            target_transform= None,\n",
    "            transforms = None,\n",
    "    ):\n",
    "        super(VOCDetection, self).__init__(root, transforms, transform, target_transform)\n",
    "        self.year = year\n",
    "        voc_root = os.path.join(self.root, \"VOC2012\")\n",
    "        image_dir = os.path.join(voc_root, 'JPEGImages')\n",
    "        annotation_dir = os.path.join(voc_root, 'Annotations')\n",
    "        \n",
    "        if image_set == \"train\":\n",
    "            self.train = True\n",
    "        else:\n",
    "            self.train = False\n",
    "\n",
    "        if not os.path.isdir(voc_root):\n",
    "            raise RuntimeError('Dataset not found or corrupted.')\n",
    "\n",
    "        splits_dir = os.path.join(voc_root, 'ImageSets/Main')\n",
    "\n",
    "        split_f = os.path.join(splits_dir, image_set.rstrip('\\n') + '.txt')\n",
    "\n",
    "        with open(os.path.join(split_f), \"r\") as f:\n",
    "            file_names = [x.strip() for x in f.readlines()]\n",
    "\n",
    "        self.images = [os.path.join(image_dir, x + \".jpg\") for x in file_names]\n",
    "        self.annotations = [os.path.join(annotation_dir, x + \".xml\") for x in file_names]\n",
    "        assert (len(self.images) == len(self.annotations))\n",
    "    \n",
    "    def transform_annotation_to_bbox(self, target):\n",
    "        class_to_ind = dict(zip(VOC_CLASSES, range(len(VOC_CLASSES))))\n",
    "        \n",
    "        res = []\n",
    "        classes = []\n",
    "        for obj in target[\"annotation\"][\"object\"]:\n",
    "            difficult = int(obj['difficult']) == 1\n",
    "            name = obj[\"name\"].lower().strip()\n",
    "            bbox = obj['bndbox']\n",
    "            pts = ['xmin', 'ymin', 'xmax', 'ymax']\n",
    "            bndbox = []\n",
    "            for i, pt in enumerate(pts):\n",
    "                cur_pt = int(bbox[pt]) - 1\n",
    "#                 cur_pt = cur_pt / width if i % 2 == 0 else cur_pt / height\n",
    "                bndbox.append(cur_pt)\n",
    "            label_idx = class_to_ind[name]\n",
    "            classes.append(label_idx)\n",
    "            res += [bndbox]  # [xmin, ymin, xmax, ymax, label_ind]\n",
    "\n",
    "        return res, classes\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is a dictionary of the XML tree.\n",
    "        \"\"\"\n",
    "        trans = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.485,0.456,0.406), (0.229,0.224,0.225))\n",
    "        ])\n",
    "        img = Image.open(self.images[index]).convert('RGB')\n",
    "        target = self.parse_voc_xml(ET.parse(self.annotations[index]).getroot())\n",
    "\n",
    "        img = trans(img)\n",
    "            \n",
    "        bndboxes, clses = self.transform_annotation_to_bbox(target)\n",
    "\n",
    "        return img, bndboxes, clses\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.images)\n",
    "\n",
    "    def parse_voc_xml(self, node):\n",
    "        voc_dict = {}\n",
    "        children = list(node)\n",
    "        if children:\n",
    "            def_dic= collections.defaultdict(list)\n",
    "            for dc in map(self.parse_voc_xml, children):\n",
    "                for ind, v in dc.items():\n",
    "                    def_dic[ind].append(v)\n",
    "            if node.tag == 'annotation':\n",
    "                def_dic['object'] = [def_dic['object']]\n",
    "            voc_dict = {\n",
    "                node.tag:\n",
    "                    {ind: v[0] if len(v) == 1 else v\n",
    "                     for ind, v in def_dic.items()}\n",
    "            }\n",
    "        if node.text:\n",
    "            text = node.text.strip()\n",
    "            if not children:\n",
    "                voc_dict[node.tag] = text\n",
    "        return voc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = VOCDetection(\"VOCdevkit\")\n",
    "# train_dataloader = data.DataLoader(train_dataset, batch_size=8, shuffle=True, num_w)\n",
    "val_dataset = VOCDetection(\"VOCdevkit\",image_set=\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-1.1247, -1.3644, -1.3130,  ..., -1.2959, -1.2103, -1.3302],\n",
       "          [-1.4672, -1.5185, -1.1589,  ..., -1.3644, -1.2445, -1.1247],\n",
       "          [-1.4158, -1.2103, -0.7993,  ..., -1.2274, -1.3815, -1.5528],\n",
       "          ...,\n",
       "          [-1.1932, -1.6555, -1.6555,  ..., -0.3883,  0.0741, -0.3369],\n",
       "          [-1.5699, -1.6384, -1.6042,  ..., -0.1314,  0.7248,  0.4337],\n",
       "          [-1.5699, -1.6213, -1.7069,  ...,  0.6221, -0.0801, -0.3027]],\n",
       " \n",
       "         [[-0.4951, -0.7752, -0.8102,  ..., -0.7227, -0.5826, -0.7052],\n",
       "          [-0.8803, -0.9678, -0.7052,  ..., -0.8102, -0.6877, -0.5651],\n",
       "          [-0.8102, -0.6352, -0.2850,  ..., -0.6702, -0.8277, -1.0028],\n",
       "          ...,\n",
       "          [-0.8452, -1.2304, -1.2479,  ..., -0.4601,  0.0476, -0.3375],\n",
       "          [-1.2479, -1.1779, -1.1954,  ..., -0.2500,  0.6954,  0.4328],\n",
       "          [-1.1954, -1.1253, -1.2479,  ...,  0.6429, -0.0224, -0.1975]],\n",
       " \n",
       "         [[-0.9330, -1.2119, -1.0898,  ..., -1.0376, -0.9504, -1.0724],\n",
       "          [-1.3513, -1.3513, -0.9504,  ..., -1.0898, -0.9678, -0.8458],\n",
       "          [-1.5604, -1.2990, -0.8284,  ..., -0.9156, -1.0724, -1.2119],\n",
       "          ...,\n",
       "          [-1.5081, -1.7870, -1.6824,  ..., -0.7936, -0.2358, -0.5670],\n",
       "          [-1.7870, -1.6999, -1.6650,  ..., -0.4973,  0.4439,  0.2522],\n",
       "          [-1.7870, -1.6650, -1.6999,  ...,  0.3742, -0.2707, -0.4624]]]),\n",
       " [[122, 50, 286, 338],\n",
       "  [49, 178, 310, 374],\n",
       "  [113, 183, 200, 370],\n",
       "  [0, 110, 94, 374]],\n",
       " [14, 1, 14, 14])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
